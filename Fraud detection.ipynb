{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb0b28-9cae-4f83-9253-ae5df6cb95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"synthetic_fraud_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237538eb-8f43-4977-8548-fac0b8566926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count how many transactions are fraud vs. not fraud\n",
    "fraud_counts = df['Fraud_Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d288a-3cd7-4942-b58c-ba80385e4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Show the count results\n",
    "print(fraud_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a0dcc2-2567-44f6-bba7-480e87295e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Plot it visually as a bar chart\n",
    "fraud_counts.plot(kind='bar', title='Fraud vs Non-Fraud Transactions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34195d52-4bd5-47e7-a724-1027377ea5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by Device_Type and Fraud_Label, then count each group\n",
    "device_fraud_counts = df.groupby(['Device_Type', 'Fraud_Label']).size()\n",
    "\n",
    "# Preview the grouped result\n",
    "print(device_fraud_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4af43-852a-4a25-be7c-e9508a7c6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_fraud_counts = device_fraud_counts.unstack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eac953-ae51-4909-8532-f847728b2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_fraud_counts.plot(kind='bar', stacked=True, title='Fraud vs Non-Fraud by Device Type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65683f23-6ad4-4139-bdbd-41a4be67595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by Location and Fraud_Label, then count\n",
    "location_fraud_counts = df.groupby(['Location', 'Fraud_Label']).size()\n",
    "\n",
    "# Step 2: Reshape with unstack\n",
    "location_fraud_counts = location_fraud_counts.unstack()\n",
    "\n",
    "# Step 3: Plot as stacked bar chart\n",
    "location_fraud_counts.plot(kind='bar', stacked=True, figsize=(10, 6),\n",
    "                           title='Fraud vs Non-Fraud by Location')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32e0a4-6f22-4a65-b527-3c622eef7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by Location and Fraud_Label, then count\n",
    "location_fraud = df.groupby(['Location', 'Fraud_Label']).size().unstack()\n",
    "\n",
    "# Step 2: Rename columns for clarity (0 = Not Fraud, 1 = Fraud)\n",
    "location_fraud.columns = ['Not_Fraud', 'Fraud']\n",
    "\n",
    "# Step 3: Calculate fraud rate as percentage\n",
    "location_fraud['Fraud_Rate_%'] = (location_fraud['Fraud'] / \n",
    "                                   (location_fraud['Fraud'] + location_fraud['Not_Fraud'])) * 100\n",
    "\n",
    "# Step 4: Sort by fraud rate (optional, for insight)\n",
    "location_fraud_sorted = location_fraud.sort_values(by='Fraud_Rate_%', ascending=False)\n",
    "\n",
    "# Step 5: Show result\n",
    "print(location_fraud_sorted[['Fraud_Rate_%']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07657f65-ab78-4623-a2c0-61ca43d24ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For working with data\n",
    "from sklearn.model_selection import train_test_split  # To split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901dfe1-d6a7-4c0c-8a3e-cf728e8f9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column - what we're trying to predict (0 = Not Fraud, 1 = Fraud)\n",
    "y = df['Fraud_Label']\n",
    "\n",
    "# Features - the inputs used to make the prediction\n",
    "X = df[['Transaction_Type', 'Device_Type', 'Location', 'Is_Weekend', 'Transaction_Amount']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4530b-1d41-4bbd-87eb-fa5c1b5e2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns into numeric dummy/indicator variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15897ed4-cfba-4d98-9ebf-13f46459a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7655b0f-5588-414d-ac84-48fbb3aaab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef03fa1-ea5d-4df8-a83a-74469dad4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3928c-ddf8-4b27-bce7-afcf76bd6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e58c42-393f-487e-aefb-b8646593b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff1254-2766-498c-89ef-d96e321a193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: How many total predictions were correct\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Precision: Of predicted frauds, how many were actually fraud\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Recall: Of all actual frauds, how many were caught\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# F1 Score: Balance between precision & recall\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion Matrix: TP, TN, FP, FN breakdown\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92969a-120d-470f-9754-1a53b50c2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a8b65-b035-4b51-bf4b-fb0f60c708ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05928f-2bc2-4955-8b81-41bf18a41fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31a7df-a34a-4ab1-9d95-9c1342b41be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d70476-ec75-4f34-9397-92f301d9ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: How many total predictions were correct\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Precision: Of predicted frauds, how many were actually fraud\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Recall: Of all actual frauds, how many were caught\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# F1 Score: Balance between precision & recall\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion Matrix: TP, TN, FP, FN breakdown\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9469de-7624-4a48-a3f1-087ad30a92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c37ec-d922-46fa-a01f-bd24783607e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create a SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE only to training data\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9ec7b-deaa-497f-a363-5313e4bba4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.Series(y_train_balanced).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea706f-bd5a-4c53-b1ef-fb7bc9dd24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_balanced, y_train_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796fa5b-8185-44ab-9238-aa03f32f1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Step 1: Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 2: Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Step 3: Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f34a61-c9df-4c82-8f1a-d8a3d44d5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define labels\n",
    "labels = ['Not Fraud', 'Fraud']\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "# Label the plot\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Fraud Detection')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b3ec8-e3e1-43e5-83a6-be3370356559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Print results\n",
    "print(\"Random Forest Performance:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Precision:\", precision_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "print(\"F1 Score:\", f1_rf)\n",
    "print(\"Confusion Matrix:\\n\", cm_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871c232-8ec4-4b90-9c2a-aea08ad6393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 2. Create the model (we'll use 5 neighbors to start)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# 3. Train the model using the SMOTE-balanced training set\n",
    "knn_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# 4. Predict using the test set\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# 5. Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "precision_knn = precision_score(y_test, y_pred_knn)\n",
    "recall_knn = recall_score(y_test, y_pred_knn)\n",
    "f1_knn = f1_score(y_test, y_pred_knn)\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "# 6. Print the results\n",
    "print(\"K-Nearest Neighbors Performance:\")\n",
    "print(\"Accuracy:\", accuracy_knn)\n",
    "print(\"Precision:\", precision_knn)\n",
    "print(\"Recall:\", recall_knn)\n",
    "print(\"F1 Score:\", f1_knn)\n",
    "print(\"Confusion Matrix:\\n\", cm_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69febc82-942c-46d6-8ad9-5fc8bece59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 2. Create the model\n",
    "svc_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# 3. Train the model on balanced data\n",
    "svc_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# 4. Predict using the test data\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "\n",
    "# 5. Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "precision_svc = precision_score(y_test, y_pred_svc)\n",
    "recall_svc = recall_score(y_test, y_pred_svc)\n",
    "f1_svc = f1_score(y_test, y_pred_svc)\n",
    "cm_svc = confusion_matrix(y_test, y_pred_svc)\n",
    "\n",
    "# 6. Print results\n",
    "print(\"Support Vector Machine (SVM) Performance:\")\n",
    "print(\"Accuracy:\", accuracy_svc)\n",
    "print(\"Precision:\", precision_svc)\n",
    "print(\"Recall:\", recall_svc)\n",
    "print(\"F1 Score:\", f1_svc)\n",
    "print(\"Confusion Matrix:\\n\", cm_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a7b82-6af4-49da-87fa-b5e1b06bebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "precision_svc = precision_score(y_test, y_pred_svc)\n",
    "recall_svc = recall_score(y_test, y_pred_svc)\n",
    "f1_svc = f1_score(y_test, y_pred_svc)\n",
    "cm_svc = confusion_matrix(y_test, y_pred_svc)\n",
    "\n",
    "# Print results\n",
    "print(\"Support Vector Machine (SVM) Performance:\")\n",
    "print(\"Accuracy:\", accuracy_svc)\n",
    "print(\"Precision:\", precision_svc)\n",
    "print(\"Recall:\", recall_svc)\n",
    "print(\"F1 Score:\", f1_svc)\n",
    "print(\"Confusion Matrix:\\n\", cm_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc5872-87a7-435a-a69f-84aea2c293b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with your results\n",
    "model_results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"K-Nearest Neighbors\", \"Support Vector Machine\"],\n",
    "    \"Accuracy\": [0.4933, 0.5491, 0.5223, accuracy_svc],\n",
    "    \"Precision\": [0.3233, 0.3277, 0.3199, precision_svc],\n",
    "    \"Recall\": [0.5181, 0.3747, 0.4235, recall_svc],\n",
    "    \"F1 Score\": [0.3981, 0.3496, 0.3645, f1_svc]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Display nicely\n",
    "print(\"📌 Model Performance Comparison:\")\n",
    "display(results_df.sort_values(by=\"F1 Score\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd53897-54d4-4934-bcdd-593299c9e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the F1 Score comparison\n",
    "results_df.plot(kind='bar', x='Model', y='F1 Score', legend=False)\n",
    "plt.title('Model Comparison - F1 Score')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xlabel('Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55dd04-3005-45f3-b175-d79670136a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbfebb-4b9a-4a53-959e-6f68861e46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Get precision-recall curve data\n",
    "precision, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Calculate AUC for the Precision-Recall curve\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, color='blue', label=f'AUC = {pr_auc:.2f}')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76abf6-d252-425d-b98a-df2d0c4754ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, 'fraud_detection_model.pkl')\n",
    "\n",
    "# Load the model back\n",
    "model_loaded = joblib.load('fraud_detection_model.pkl')\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred_loaded = model_loaded.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50108eb-cf46-43cf-825b-d058d636e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try importing all the libraries you need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79a57c-c113-40ce-8a2e-50756d970155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
